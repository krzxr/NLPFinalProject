# When Does BERT Work? An Error Analysis in Author Attribution with respect to Topic and Style

The board question we are interested in is to what extent does BERT rely on style and content features (as defined in Sari 2018, Topic or Style?). As such, we fine tuned Bert on a balanced 2-authors dataset that we extracted from the blogger dataset. Then we modify the dataset to remove style features capitalization and punctuation, and content features bigrams and higher n-grams. Evaluating BERT on these modified dataset, we find that in this specific case, BERT relies on punctuation, capitalization, and n-gram in increasing order! We further fine additional Bert models on these modified datasets. We demonstrate that these variants underperform as expected, and we explore what might cause them to underperform from a training perspective. These findings help us to better open the black box of and gain understanding in BERT.
